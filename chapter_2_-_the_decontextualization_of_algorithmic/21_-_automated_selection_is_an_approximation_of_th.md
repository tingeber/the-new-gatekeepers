## 2.1 - Automated selection is an approximation of the human process {#2-1-automated-selection-is-an-approximation-of-the-human-process}

In previous paragraphs, we discussed the separation of process and context in the process of algorithmic creation. However, the creation and selection of appropriate sorting and categorizing functions for a certain dataset does take into account an exemplifying part of the content it is meant to categorize. When testing efficiency of an algorithmic process, developers turn to “**training samples**,” or subsets of datasets those algorithms will be used to filter. The training samples are usually chosen randomly, with varying levels of human manipulation, and are considered representative of the larger dataset. The utilization of training samples is necessary, again from the viewpoint of efficiency, because fine-tuning algorithms on entire datasets in most cases isn’t cost-effective. In fact, the usefulness of algorithmic filtering processes is directly related to the size of the dataset — if we follow our initial statement that algorithms are required to filter quantities of information that individuals aren’t physically capable to process, the larger a dataset, the more important it is to be able to automatically filter and categorize it. Thus, the importance of automatization grows as the datasets grow.

We will define two issues with training samples: **temporal staticity** and **majority rule**. Temporal staticity means that a training sample is extracted from a set of data at a certain point in time, and the highest categorizing efficiency will be defined by what information was represented at that point. While algorithms are regularly fine-tuned, either automatically (with consequences detailed in chapter 3) or through human intervention, it is entirely possible that some initial assumptions stop being relevant without anyone noticing or remembering they were there in the first place. It is important to remember that for the purposes of this paper “algorithm” is shorthand, or synecdoche, for extremely complex systems of automated sorting, categorizing, and prioritizing mechanisms that are ever-changing, and are being built, rebuilt and added upon constantly.

_Programmers do not, and often cannot, predict what their complex programs will do. Google’s Internet services are billions of lines of code. Once these algorithms with an enormous number of moving parts are set loose, they then interact with the world, and learn and react. The consequences aren’t easily predictable._ [_(Tufekci, 2016, para. 12)_](https://paperpile.com/c/BG18Wg/BnMg/?locator_label=paragraph&locator=12)

Many parts of algorithms are discrete chunks of code, sometimes referred to as “libraries” that are used and reused by many developers because they satisfy a purpose without reinventing the wheel (some basic examples are geographic autocomplete libraries, or libraries that collect sets of the most common functions). This ads dimensions out of the developers’ or the owners’ control to the complexity of an algorithmic process, and serves to illustrate the difficulty to keep track of every choice ever made — especially if an algorithm apparently satisfies criteria of efficiency. Majority rule is the issue of over-representation of voices of the majority in the training samples, that therefore create algorithms that are particularly well suited for representatives of the majority. While this makes logical, rational sense, it is an issue to keep in mind when considering that those same algorithms serve everyone, minorities included. We will analyse some aspects of majority rule in chapter 3.